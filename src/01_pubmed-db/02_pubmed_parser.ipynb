{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:30px\" align=\"center\"> <b> Downloading MEDLINE/PubMed Data and Posting to PostgreSQL </b> </div>\n",
    "\n",
    "<div style=\"font-size:18px\" align=\"center\"> <b> Brandon L. Kramer - University of Virginia's Bicomplexity Institute </b> </div>\n",
    "\n",
    "<br>\n",
    "\n",
    "This notebook detail the process of downloading all of [PubMed's MEDLINE data](https://www.nlm.nih.gov/databases/download/pubmed_medline.html) and posting it to a PostgresSQL database ([UVA's Rivanna OpenOnDemand](https://rivanna-portal.hpc.virginia.edu/)). To do this, we use the terminal to download all of the data into Rivanna. Next, we use the [PubMedPortable](https://github.com/KerstenDoering/PubMedPortable) package through the Python shell to parse all of the data and build up a database. \n",
    "\n",
    "### Step 1: Download PubMed Data\n",
    "\n",
    "First, we download all of the data from [here](ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline/) using `wget`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /scratch/kb7hp/pubmed_new \n",
    "wget --recursive --no-parent ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Download PubMedPortable\n",
    "\n",
    "Second, we will clone [PubMedPortable package from GitHub](https://github.com/KerstenDoering/PubMedPortable). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/kb7hp/git/\n",
    "git clone https://github.com/KerstenDoering/PubMedPortable.git\n",
    "cd PubMedPortable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Populate Tables in PostgreSQL Database \n",
    "\n",
    "Go to the [PubMedPortable](https://github.com/KerstenDoering/PubMedPortable/wiki#build-up-a-relational-database-in-postgresql) protocol:\n",
    "  - Skip the part on making a superuser named parser and use Rivanna login and pwd instead \n",
    "  - Since `PubMedPortable` is written with the login/pwd of parser/parser you have to update lines 704-750 of `PubMedDB.py` \n",
    "      - Add `import psycopg2 as pg` to the beginning of the file\n",
    "      - Update all the connections to: `con = 'postgresql+psycopg2://login:pwd@postgis1/sdad'`\n",
    "      - Update all the `print` statements to `print()` (e.g. line 728)\n",
    "\n",
    "Go to [Rivanna OpenOnDemand](https://rivanna-portal.hpc.virginia.edu/), click on Clusters > Rivanna Shell Access and then create a new schema using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U login -d sdad -h postgis1\n",
    "CREATE SCHEMA pubmed_2021; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then return to the Python terminal and run this to populate the new schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/kb7hp/git/PubMedPortable\n",
    "python PubMedDB.py -d pubmed_2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the Rivanna PostgreSQL shell to check if that worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\dt pubmed_2021.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it did so now we can start parsing.\n",
    "\n",
    "### Step 4: Testing MEDLINE Data Upload \n",
    "\n",
    "We don't want to start dumping all 1062 files, so let's just start with one. We will create a pm_0001 folder and download just one of the .xml files from PubMed. Next, we had to debug the `PubMedParser.py` file by updating all of the `con` and `print` statements as we did above and update `_next` to `__next__` on line 65. After doing this, we ran the following code to upload our first test file.\n",
    "\n",
    "#### Batch 1 (0001)\n",
    "\n",
    "Let's give this a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/kb7hp/git/PubMedPortable/data\n",
    "mkdir pm_0001 \n",
    "cd pm_0001\n",
    "wget ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline/pubmed21n0001.xml.gz\n",
    "cd /home/kb7hp/git/PubMedPortable/\n",
    "python PubMedParser.py -i data/pm_0001/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took about 8 minutes to run this one file. \n",
    "\n",
    "### Step 5: Uploading the Rest of the MEDLINE Dataset to PostgreSQL Database in Batches \n",
    "\n",
    "Let's add the rest of the data to Postgres. Ideally, we would just dump the whole thing at once, but Rivanna limits the amount of data we can store locally (for some reason `PubMedPortable` does not like absolute paths). Thus, we will only copy part of the data from the `\\scratch` folder to our temporary local folders.\n",
    "\n",
    "#### Batch 2 (0002-0011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move all the .xml.gz files to their own folder\n",
    "cd /scratch/kb7hp/\n",
    "mkdir pubmed_gz\n",
    "cd /scratch/kb7hp/pubmed_new/ftp.ncbi.nlm.nih.gov/pubmed/baseline/\n",
    "mv *.gz /scratch/kb7hp/pubmed_gz\n",
    "\n",
    "# and copy 10 of those files to that new folder \n",
    "cd /scratch/kb7hp/pubmed_gz/\n",
    "cp pubmed21n{0002..0011}.xml.gz /home/kb7hp/git/PubMedPortable/data/pm_0002_0011\n",
    "\n",
    "# and then we add those 10 files to our existing database \n",
    "cd /home/kb7hp/git/PubMedPortable/data/\n",
    "python PubMedParser.py -i data/pm_0002_0011/ -c -p 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I intially thought this process would take ~80 minutes, running these 10 files only look ~22 minutes because of the 4 cores that essentially cut the timing by a quarter. Thus, we spun an instance with 5 cores (1 extra as directed by the Rivanna admins) and ran the next ~90 files with this new allocation. When I checked the `pubmed_2021.tbl_abstract` table, we had 146,854 rows, which seemed low. Yet, the notification from the `PubMedParser.py` file indicated that all files were parsed. I would late come to realize that there are fewer abstracts than total records, which can be validated in the `pubmed_2021.tbl_medline_citation` table. \n",
    "\n",
    "#### Batch 3 (0012-0100)\n",
    "\n",
    "Let's dump the next batch of citations (0012-0100). We will copy over the next batch of data and with multiprocessing accounted for this should take ~3 hours to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /scratch/kb7hp/pubmed_gz/\n",
    "cp pubmed21n{0012..0100}.xml.gz /home/kb7hp/git/PubMedPortable/data/pm_0012_0100\n",
    "cd /home/kb7hp/git/PubMedPortable/data/\n",
    "python PubMedParser.py -i data/pm_0012_0100/ -c -p 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed it did! We have loaded the first 100 files and it took just over 3 hours (13:19:19-16:22:52). \n",
    "\n",
    "#### Batch 4 (0101-0500)\n",
    "\n",
    "Now, let's get a bit more ambitious. Given its now night time, we are can boost the allocation to 9 cores and try ~400 files. This should take around around 7 hours to complete (400 files * 8 mins/file with 8 cores). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will clean up the local directory \n",
    "cd /home/kb7hp/git/PubMedPortable/data/\n",
    "rm -r pm_0001 \n",
    "rm -r pm_0002_0011\n",
    "rm -r pm_0012_0100\n",
    "\n",
    "# copy over our new files \n",
    "cd /scratch/kb7hp/pubmed_gz \n",
    "cp pubmed21n{0101..0500}.xml.gz /home/kb7hp/git/PubMedPortable/data/pm_0101_0500\n",
    "\n",
    "# and then run the script for the next 400 files \n",
    "cd /home/kb7hp/git/PubMedPortable/\n",
    "python PubMedParser.py -i data/pm_0101_0500/ -c -p 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After parsing the pm_101_500 files, I woke up to a minor error, but it looks like the program continued running up through the very last citation of the last file. I checked the `pubmed_2021.tbl_abstract` table and had 6,388,959 entries while `pubmed_2021.tbl_medline_citation` had 13,095,000, which almost half of the 26 million advertised on [MEDLINE's website](https://www.nlm.nih.gov/bsd/medline.html). Thus, it does seem like everything parsed without any serious problems. I decided to finsih up the rest of the file parsing since (1) I cannot address any problem in a systematic way and (2) a full database with problems is still better than a half database with problems. \n",
    "\n",
    "#### Batch 5 (0501-0750)\n",
    "\n",
    "With the space limitations, let's take a conservative approach and post the next 250 files to the database (once again using 9 cores on Rivanna). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/kb7hp/git/PubMedPortable/data\n",
    "rm -r pm_0101_0500\n",
    "mkdir pm_0501_0750\n",
    "cd /scratch/kb7hp/pubmed_gz \n",
    "cp pubmed21n{0501..0750}.xml.gz /home/kb7hp/git/PubMedPortable/data/pm_0501_0750\n",
    "cd /home/kb7hp/git/PubMedPortable/\n",
    "python PubMedParser.py -i data/pm_0501_0750/ -c -p 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took just over 4 hours (08:34:23-13:00:31) and worked flawlessly (no errors whatsoever). At this point, we have 12,158,748 abstracts in the `pubmed_2021.tbl_abstract` table. \n",
    "\n",
    "#### Batch 6 (0751-0900)\n",
    "\n",
    "While I thought this would be the last batch, I ran out of space again trying to dump 750-1062. Let's do up to 900 and do the last batch later today. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/kb7hp/git/PubMedPortable/data\n",
    "rm -r pm_0501_0750\n",
    "mkdir pm_0751_0900\n",
    "cd /scratch/kb7hp/pubmed_gz\n",
    "cp pubmed21n{0751..0900}.xml.gz /home/kb7hp/git/PubMedPortable/data/pm_0751_0900\n",
    "cd /home/kb7hp/git/PubMedPortable/\n",
    "python PubMedParser.py -i data/pm_0751_0900/ -c -p 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took __ hours and once again ran without errors. \n",
    "\n",
    "#### Batch 7 (0901-1062)\n",
    "\n",
    "We dumped the last batch with this code and we were done! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/kb7hp/git/PubMedPortable/data\n",
    "rm -r pm_0751_0900\n",
    "mkdir pm_0901_1062\n",
    "cd /scratch/kb7hp/pubmed_gz\n",
    "cp pubmed21n{0901..1062}.xml.gz /home/kb7hp/git/PubMedPortable/data/pm_0901_1062\n",
    "cd /home/kb7hp/git/PubMedPortable/\n",
    "python PubMedParser.py -i data/pm_0751_1062/ -c -p 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On to the Next Step in Your Research Project\n",
    "\n",
    "Overall, this was a surprisingly easy process. A major kudos goes out to PubMedPortable for this fantastic package. Now, let's get to text mining!  \n",
    "\n",
    "### References \n",
    "\n",
    "Döring, K., Grüning, B. A., Telukunta, K. K., Thomas, P., & Günther, S. (2016). PubMedPortable: a framework for supporting the development of text mining applications. Plos one, 11(10), e0163794. (Link to [Article](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0163794) and [GitHub Repo](https://github.com/KerstenDoering/PubMedPortable))\n",
    "\n",
    "National Library of Medicine. (2021). Download MEDLINE/PubMed Data. Link to [Data](ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline) and [Documentation](https://www.nlm.nih.gov/databases/download/pubmed_medline.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
