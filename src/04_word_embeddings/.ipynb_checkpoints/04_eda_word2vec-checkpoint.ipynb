{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:30px\" align=\"center\"> <b> Visualizing Word2Vec Models Trained on Biomedical Abstracts in PubMed </b> </div>\n",
    "<div style=\"font-size:22px\" align=\"center\"> <b> A Comparison of Race and Diversity Over Time </b> </div>\n",
    "<br>\n",
    "\n",
    "<div style=\"font-size:18px\" align=\"center\"> <b> Brandon L. Kramer - University of Virginia's Bicomplexity Institute </b> </div>\n",
    "\n",
    "<br>\n",
    "\n",
    "This notebook explores two Word2Vec models trained the PubMed database taken from January 2021. Overall, I am interested in testing whether diversity and racial terms are becoming more closely related over time. To do this, I [trained](https://github.com/brandonleekramer/diversity/blob/master/src/04_word_embeddings/03_train_word2vec.ipynb) two models (one from 1990-1995 data and then a random sample of the 2015-2020 data). Now, I will visualize the results of these models to see which words are similar to race/diversity as well as plotting some comparisons of these two terms over time.\n",
    "\n",
    "For those unfamiliar with Word2Vec, it might be worth reading [this post from Connor Gilroy](https://ccgilroy.github.io/community-discourse/introduction.html) - a sociologist that details how word embeddings can help us better understand the concept of \"community.\" The post contains information on how Word2Vec and other word embedding approaches can teach us about word/document similarity, opposite words, and historical changes in words. Basically, Word2Vec turns all of the words in the corpus into a number based on how they are used in the context of sentences, making all of the words directly compariable to one another within a vector space. The end result is that we are able to compare how similar or different words are or, as we will see below, how similar or different words become over time. \n",
    "\n",
    "#### Import packages and ingest data \n",
    "\n",
    "Let's load all of our packages and the `.bin` files that hold our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import os\n",
    "import pandas.io.sql as psql\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# load data \n",
    "os.chdir(\"/sfs/qumulo/qhome/kb7hp/git/diversity/data/word_embeddings/\")\n",
    "earlier_model = Word2Vec.load(\"word2vec_1990_95.bin\")\n",
    "later_model = Word2Vec.load(\"word2vec_2015_20.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing Most Similar Words \n",
    "\n",
    "What words are most similar to \"racial\" and \"diversity\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "racial_sim_early = earlier_model.wv.most_similar('racial')\n",
    "print(\"In the 1990-1995 model, 'racial' is similar to:\")\n",
    "racial_sim_early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "racial_sim_later = later_model.wv.most_similar('racial')\n",
    "print(\"In the 2015-2020 model, 'racial' is similar to:\")\n",
    "racial_sim_later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity_sim_early = earlier_model.wv.most_similar('diversity')\n",
    "print(\"In the 1990-1995 model, 'diversity' is similar to:\")\n",
    "diversity_sim_early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity_sim_later = later_model.wv.most_similar('diversity')\n",
    "print(\"In the 2015-2020 model, 'diversity' is similar to:\")\n",
    "diversity_sim_later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, \"racial\" is mostly similar to other racialized and/or gendered terms. \"Diversity\", on the other hand, is most similar to heterogeneity and a number of terms more generally classified under differences and/or complexity. That makes it a little difficult to directly compare the terms, so let's use the `wv.similarity` function to directly look at that.\n",
    "\n",
    "#### Comparing Race and Diversity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing racial and diversity:\n",
      "The 1990-1995 score is: 0.22878885\n",
      "The 2015-2020 score is: 0.19454905\n",
      "The overall difference is: 0.0342398\n",
      "Comparing race and diversity:\n",
      "The 1990-1995 score is: 0.10367876\n",
      "The 2015-2020 score is: 0.08713666\n",
      "The overall difference is: 0.0165421\n",
      "Comparing ethnic and diversity:\n",
      "The 1990-1995 score is: 0.26469263\n",
      "The 2015-2020 score is: 0.24086899\n",
      "The overall difference is: 0.023823649\n",
      "Comparing ethnicity and diversity:\n",
      "The 1990-1995 score is: 0.17010239\n",
      "The 2015-2020 score is: 0.14082599\n",
      "The overall difference is: 0.0292764\n"
     ]
    }
   ],
   "source": [
    "racial_diversity_early = earlier_model.wv.similarity('racial','diversity')\n",
    "racial_diversity_later = later_model.wv.similarity('racial','diversity')\n",
    "race_diversity_early = earlier_model.wv.similarity('race','diversity')\n",
    "race_diversity_later = later_model.wv.similarity('race','diversity')\n",
    "ethnic_diversity_early = earlier_model.wv.similarity('ethnic','diversity')\n",
    "ethnic_diversity_later = later_model.wv.similarity('ethnic','diversity')\n",
    "ethnicity_diversity_early = earlier_model.wv.similarity('ethnicity','diversity')\n",
    "ethnicity_diversity_later = later_model.wv.similarity('ethnicity','diversity')\n",
    "\n",
    "print('Comparing racial and diversity:')\n",
    "print('The 1990-1995 score is:', racial_diversity_early)\n",
    "print('The 2015-2020 score is:', racial_diversity_later)\n",
    "print('The overall difference is:', racial_diversity_early - racial_diversity_later)\n",
    "print('Comparing race and diversity:')\n",
    "print('The 1990-1995 score is:', race_diversity_early)\n",
    "print('The 2015-2020 score is:', race_diversity_later)\n",
    "print('The overall difference is:', race_diversity_early - race_diversity_later)\n",
    "print('Comparing ethnic and diversity:')\n",
    "print('The 1990-1995 score is:', ethnic_diversity_early)\n",
    "print('The 2015-2020 score is:', ethnic_diversity_later)\n",
    "print('The overall difference is:', ethnic_diversity_early - ethnic_diversity_later)\n",
    "print('Comparing ethnicity and diversity:')\n",
    "print('The 1990-1995 score is:', ethnicity_diversity_early)\n",
    "print('The 2015-2020 score is:', ethnicity_diversity_later)\n",
    "print('The overall difference is:', ethnicity_diversity_early - ethnicity_diversity_later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like in each case the scores drops, signifying that race and ethnicity are both becoming conceptually closer to diversity over time.\n",
    "\n",
    "#### Analyzing Analogies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_racism = earlier_model.wv.most_similar(positive=['black', 'racism'], negative=['white'], topn=20)\n",
    "white_racism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_racist = earlier_model.wv.most_similar(positive=['white', 'racist'], negative=['black'], topn=20)\n",
    "black_racist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "earlier_vocab = list(earlier_model.wv.vocab)\n",
    "earlier_x = earlier_model[earlier_vocab]\n",
    "earlier_tsne = TSNE(n_components=2)\n",
    "earlier_tsne_x = earlier_tsne.fit_transform(earlier_x)\n",
    "df_earlier = pd.DataFrame(earlier_tsne_x, index=earlier_vocab, columns=['x', 'y'])\n",
    "\n",
    "keys = ['race', 'racial', 'ethnic', 'ethnicity', 'diverse', 'diversity']\n",
    "\n",
    "earlier_embedding_clusters = []\n",
    "earlier_word_clusters = []\n",
    "for word in keys:\n",
    "    earlier_embeddings = []\n",
    "    earlier_words = []\n",
    "    for similar_word, _ in earlier_model.wv.most_similar(word, topn=30):\n",
    "        earlier_words.append(similar_word)\n",
    "        earlier_embeddings.append(earlier_model[similar_word])\n",
    "    earlier_embedding_clusters.append(earlier_embeddings)\n",
    "    earlier_word_clusters.append(words)\n",
    "    \n",
    "earlier_embedding_clusters = np.array(earlier_embedding_clusters)\n",
    "n, m, k = earlier_embedding_clusters.shape\n",
    "e_tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "e_embeddings_en_2d = np.array(e_tsne_model_en_2d.fit_transform(earlier_embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "later_vocab = list(later_model.wv.vocab)\n",
    "later_x = later_model[later_vocab]\n",
    "later_tsne = TSNE(n_components=2)\n",
    "later_tsne_x = later_tsne.fit_transform(later_x)\n",
    "df_later = pd.DataFrame(later_tsne_x, index=later_vocab, columns=['x', 'y'])\n",
    "\n",
    "later_embedding_clusters = []\n",
    "later_word_clusters = []\n",
    "for word in keys:\n",
    "    later_embeddings = []\n",
    "    later_words = []\n",
    "    for similar_word, _ in later_model.wv.most_similar(word, topn=30):\n",
    "        later_words.append(similar_word)\n",
    "        later_embeddings.append(later_model[similar_word])\n",
    "    later_embedding_clusters.append(later_embeddings)\n",
    "    later_word_clusters.append(words)\n",
    "    \n",
    "later_embedding_clusters = np.array(later_embedding_clusters)\n",
    "n, m, k = later_embedding_clusters.shape\n",
    "l_tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "l_embeddings_en_2d = np.array(l_tsne_model_en_2d.fit_transform(later_embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot_similar_words(title, labels, earlier_embedding_clusters, earlier_word_clusters, a, filename=None):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    for label, earlier_embeddings, earlier_words, color in zip(labels, earlier_embedding_clusters, earlier_word_clusters, colors):\n",
    "        x = earlier_embeddings[:, 0]\n",
    "        y = earlier_embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=8)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    if filename:\n",
    "        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "test =tsne_plot_similar_words('Comparing the Use of Race, Ethnicity and Diversity (PubMed 1990-1995)', \n",
    "                        keys, e_embeddings_en_2d, earlier_word_clusters, \n",
    "                        0.7, 'earlier_comparison.png')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/sfs/qumulo/qhome/kb7hp/git/diversity/data/word_embeddings/\")\n",
    "plt.savefig('earlier_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot_similar_words(title, labels, later_embedding_clusters, later_word_clusters, a, filename=None):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    for label, later_embeddings, later_words, color in zip(labels, later_embedding_clusters, later_word_clusters, colors):\n",
    "        x = later_embeddings[:, 0]\n",
    "        y = later_embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=8)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    if filename:\n",
    "        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "tsne_plot_similar_words('Comparing the Use of Race, Ethnicity and Diversity (PubMed 2015-2020)', \n",
    "                        keys, l_embeddings_en_2d, later_word_clusters, \n",
    "                        0.7, 'later_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/sfs/qumulo/qhome/kb7hp/git/diversity/data/word_embeddings/\")\n",
    "plt.savefig('later_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_earlier.to_csv(\"/sfs/qumulo/qhome/kb7hp/git/diversity/data/word_embeddings/pubmed_earlier.csv\")\n",
    "df_later.to_csv(\"/sfs/qumulo/qhome/kb7hp/git/diversity/data/word_embeddings/pubmed_later.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "[Connor Gilroy's Tutorial](https://ccgilroy.github.io/community-discourse/word-similarity.html)\n",
    "[Dominiek Ter Heide's Word2Vec Explorer](https://github.com/dominiek/word2vec-explorer)\n",
    "[Sergey Smetanin's Medium Tutorial](https://towardsdatascience.com/google-news-and-leo-tolstoy-visualizing-word2vec-word-embeddings-with-t-sne-11558d8bd4d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brandon_env",
   "language": "python",
   "name": "brandon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
