---
title: "The Growth of Diversity"
author: "Brandon L. Kramer (UVA, Biocomplexity Institute) & Catherine Lee (Rutgers, Sociology)"
output: html_document
---

### Project Overview

Over the course of the 25 years, there have been persistent calls from both governmental and scientific circles to include more diverse populations in biomedical research (Epstein 2008). Among the many initatives that have been put in place since, the National Institute of Health (NIH) spearheaded this process mandating that all federally funded clinical research work to include women and racial/ethinic minorities in their research, citing concerns over population-specific diseases and potential side-effects that may only impact certain groups (Oh et al. 2015). In this project, we are interested in researching how this policy and other inclusion-based initiatives have contributed to the growth of diversity-related terminiology in biomedical research. To do this, we will be using aspects of computational text analysis to outline the emergence of diversity-related terms and how they vary by some common co-variates such as geography and discipline. 

### Data Overview

In our analyses below, we are examining the growth of in the use of the term "diversity". To do this, we drew from the MEDLINE database in Web of Science, using the search terms ["TS=(diversity)" from 1990-2017 for human research only](https://docs.google.com/spreadsheets/d/1RNpPAyowuDfA5p8Sx7nEIw1a7Z8VWFoMFc3WPQ0QELw/edit#gid=933487936). This search provided 71,528 total results, which we extracted using the bibliometrix package in R (Aria and Cuccurillo 2017). Next, we converted the abstracts of these articles to a text corpus and then used tidytext - a package designed for computational text analysis in the open-source software R (Silge and Robinson 2016) - to analyze patterns with the abstracts of these data. Below is a summary of the results, but the replication code can be found on [Brandon's Github page](github.com/brandonleekramerdiversity).

```{r setup, include = FALSE}
rm(list = ls())
#knitr::opts_knit$set(root.dir = "~/Desktop/Diversity/Data")
knitr::opts_knit$set(root.dir = "C:/Users/bkram/CloudStation/Diversity/Data")
```
```{r load packages, results = FALSE, message = FALSE, results = FALSE}
#loading packages
for (pkg in c("tidyverse", "igraph", "tidytext", "ggplot2", "ggraph", 
              "widyr", "plotly", "stringi", "networkD3")) {library(pkg, character.only = TRUE)}
# pulling the data 
text_data <- read_csv("historical_text_data.csv") %>% filter(year != "2018")
```

```{r abstract text analysis, messages = FALSE, warning=FALSE}
# tokenizing the abstract data into words 
abstract_data <- text_data %>% 
  unnest_tokens(word, abstract) %>% 
  anti_join(stop_words)
# adding custom set of stopwords 
my_stopwords <- tibble(word = c(as.character(1:9), 
                                "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", 
                                "rights", "reserved", "copyright", "elsevier"))
abstract_data <- abstract_data %>% anti_join(my_stopwords); rm(my_stopwords)
```

### Text Networks of Diversity-Related Terminology

We were also interested in the terms that most commonly occured alongside diversity. We can do this by running pairwise counts across the abstracts and then using interactive network analysis to plot what those relationships look like. In the graph presented below, the nodes correspond to commonly occuring words in our abstract dataset with the strength of the lines between the nodes aligning with the frequency those words arise in the same abstract. 

```{r co-occurence networks of diversity abstracts part 1, message = FALSE, warning = FALSE, results = FALSE, fig.width=9.5}
# pulling the population terms data
all_population_terms <- read_csv("population_terms.csv")
all_population_terms <- paste(c("\\b(?i)(zxz", all_population_terms$term, "zxz)\\b"), collapse = "|")
all_population_terms
# and creating bigraphs of words that occur more that 80 times 
filtered_pairs <- abstract_data %>%
  filter(str_detect(word, all_population_terms)) %>% 
  pairwise_count(word, id, sort = TRUE, upper = FALSE) %>% 
  filter(n >= 80)
```

Here is that massive list of population terms we mentioned above. Keep in mind that when we searched through our corpus, we ignored cases so don't mind the variation in capitalization here. 

```{r co-occurence networks of diversity abstracts part 2, fig.width=9.5, fig.height=7}
# network visualization of most frequent pairs 
set.seed(1234)
filtered_pairs %>%
    filter(n >= 80) %>%   # may need to alter this number for a cutoff point 
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
    geom_node_point(size = 3) +
    geom_node_text(aes(label = name), repel = TRUE, 
                   point.padding = unit(0.2, "lines")) + theme_void()
```

This network is composed of nodes (corresponding to poppulation terms in our corpus) and ties (representing how many times they co-occur in the same abstracts of our dataset). As you can see in the graph, there is a core that forms with the most commonly occuring words in the corpus. Below, I have made the same network interactive using the networkD3 package. This allows you to hover over each node (or word) and see which other words it is connected to in the network. 

```{r interactive co-occurence networks of diversity abstracts, fig.width=9.5, fig.height=7}
edgeList <- filtered_pairs %>% 
  rename(word1 = item1, word2 = item2, weight = n)

simpleNetwork(edgeList,  
              fontSize = 20,  
              linkDistance = 300, 
              opacity = 0.8, 
              height = 500,
              width = 1000, 
              fontFamily = "sans-serif", 
              charge = -20, 
              zoom = TRUE)
```

At first blush, this network has some mixed utility. In network terms, this is a "hairball" that shows very little about the overall structure of the relations between words. However, when you hover over the words you can see the relations between some words and not others, which is determined by a threshold of how many times two words occur together in the overall corpus. In this case, I set that threshold to 80 times, but we could adjust this to any specific cutoff we decided upon. Remember, this interactive network is the same as the one above it, but the original one adds some edge weights to know often those terms arise together in raw counts. 

There are some really great examples of how some terms relate and how others do not relate. As Catherine pointed out, you can hover over "racial" near the top and see Hollinger's classic argument about the "ethno-racial pentagon" in *Postethnic America*. Here, the term "racial" is linked to four-fifths of the terms in this pentagon (i.e. White, Black/African, Asian, and Hispanic) as well as race, ethnic, ethnicity and America. The notable exception to the pentagon is "Native," which does arise near the top of the graph but does not co-occur at least 80 times with racial, but does arise at least that many times with a host of other words.

We also we broad variation with the "reach" of continental and biogeographical terms like Asian, African, European, American, east and west. On average, these terms are connected to more terms than more specific terms, likely demonstrating their generality to tie a broader range of terms and/or groups together. When we think about this in context of our historical trends, we also know that these continential and biogeographical terms are used much more often now that racial or ethnic terms are. In other words, population contrasts are made using these more general terms rather racial/ethnic terminology. 

In contrast to these continental and biogeographical terms, we also have specific nation-states that are mentioned such as Brazil, Japan, Canada, Australia, Mexico, Korea, and Saudi Arabia. Generally, these terms are relatively unconnected to other terms in the graph. They really only arise with variations of the same terms and thus would likely fall out of the graph completely if we recoded and/or stemmed these words earlier. 

### Using Community Detection to Examine Word Clustering

We can also use an algorithm that breaks networks into neighborhoods of words that share the most frequent co-occurrences. To do this, we need to use the forceNetwork function in networkD3, which takes a little work, but also helps us see the degree centrality, betweenness centrality and dice similarity of each term in the network. 

```{r make network}
# make a graph and remove duplicate edges/self-loops 
g <- igraph::simplify(igraph::graph.data.frame(edgeList, directed=FALSE))
# extract the nodelist 
nodeList <- data.frame(ID = c(0:(igraph::vcount(g) - 1)), nName = igraph::V(g)$name)
# assign the node names from the edge list to the nodelist 
getNodeID <- function(x){
  which(x == igraph::V(g)$name) - 1 # to ensure that IDs start at 0
}
# and add them back to edgelist
edgeList <- plyr::ddply(edgeList, .variables = c("word1", "word2", "weight"), 
                        function (x) data.frame(word1ID = getNodeID(x$word1), 
                                                word2ID = getNodeID(x$word2)))
# calculate degree centrality 
nodeList <- cbind(nodeList, 
                  nodeDegree=300*igraph::degree(g, v = igraph::V(g), mode = "all"))
# calculate betweenness centrality 
betAll <- igraph::betweenness(g, v = igraph::V(g), 
          directed = FALSE) / (((igraph::vcount(g) - 1) * (igraph::vcount(g)-2)) / 2)
betAll.norm <- (betAll - min(betAll))/(max(betAll) - min(betAll))
nodeList <- cbind(nodeList, nodeBetweenness=1000*betAll.norm) 
# calculate dice similarities between all pairs of nodes 
dsAll <- igraph::similarity.dice(g, vids = igraph::V(g), mode = "all")
F1 <- function(x) {data.frame(diceSim = dsAll[x$word1ID +1, x$word2ID + 1])}
edgeList <- plyr::ddply(edgeList, 
                        .variables=c("word1", "word2", "weight", "word1ID", "word2ID"), 
                           function(x) data.frame(F1(x)))
# calculate group membership   
wc <- cluster_walktrap(g)
members <- membership(wc)
clusters <- igraph_to_networkD3(g, group = members)
nodeList <- cbind(nodeList, cluster=clusters$nodes$group) 
# and clean things up a little after we are done 
rm(betAll, betAll.norm, dsAll, F1, getNodeID, clusters)
```

```{r visualizing degree centrality}
# creating a color palette for data types 
my_colors <- 'd3.scaleOrdinal() .domain(["1", "2", "3", "4", "5", 
                                         "6", "7", "8", "9", "10",
                                         "11", "12", "13", "14", "15", "16"])
                                .range([ "#005b96", "#851e3e", "#35a79c", "#851e3e", 
                                         "#851e3e", "#851e3e", "#851e3e", "#851e3e", 
                                         "#851e3e", "#851e3e", "#851e3e", "#851e3e", 
                                         "#851e3e", "#851e3e", "#851e3e", "#851e3e"])'
# visualizing the network 
networkD3::forceNetwork(Links = edgeList, Nodes = nodeList, Source = "word1ID", Target = "word2ID", 
                        Value = "weight", NodeID = "nName", Nodesize = "nodeDegree",  
                        opacity = 0.9, opacityNoHover = 0.05, height = 500, width = 910,  
                        Group = "cluster", fontSize = 280, fontFamily = "sans-serif",
                        linkDistance = networkD3::JS("function(d) { return 8.5*d.value; }"),
                        linkWidth = JS("function(d) { return Math.sqrt(d.value / 5); }"), 
                        zoom = TRUE, charge = -20, colourScale = my_colors) 
```

**IMPORTANT** In order to see these networks, you need to scroll on the visualization. This might take a couple minutes to get used to. Just remember, if you lose the network in visualization field, you can always refresh the page to start over.  

Now, we can visualize the network in a completely different way. This network changes the size of the nodes to correspond to how densely connected they are to the other terms in the network. In this first network, the largest nodes are the most connected while the the smallest have the least connections. The width of the lines corresponds to how frequently they occur in the corpus (though I have adjusted the thickness for visual purposes). 

You also see there are three colors, which correspond to "groups" of words based on the cluster walktrap community detection algorithm (Pons and Latapy 2006) I ran on the network's relations. Truth be told, there were actually 16 "communities" detected using this algorithm, but when I went through to interpret these results I found there were actually only three groupings. (I have listed these three groups below in the Appendices.) 

Interestingly enough, these **blue** nodes map on almost *exactly* to Hollinger's pentagon (i.e. White, Black/African, Asian, Latino/Hispanic and Native) as well as race, ethnic, ethnicity and America. The only caveat here is that it also includes "Pacific" (as in Pacific Islander), which is obviously is another frequently occuring US Census category.Even more interesting is that the second trend I discussed above becomes even more clear. In the graph, the **green** nodes tend to overlap with more general geographic or continental terminology (i.e. Asian, African, European, American, east and west along with India now included in that cluster). 

The other big grouping (i.e. the nodes in **deep red**) correspond to specific nation-states like Brazil, Japan, Canada, Australia, Mexico, Korea, and Saudi Arabia. Now, one might criticize that this general "group" is actually not one algorithmically-linked cluster (and they are right!). Instead, these are actually 14 different clusters derived from the community detection algorithm. That said, the general trend here is still the same. These 14 clusters are actually an "archipelago" of terms that are all deeply inter-related without being directly connected in terms of network linkages. 

### Examining the Betweenness of Population Terminology 

The next step is changing the size of the nodes to correspond to the betweenness centrality rather than the degree centrality (without changing any of the colors from our previous network). Degree centrality is essentially just the number of times that one term was mentioned with another term. Betweenness centrality, on the other hand, generally helps us detect nodes that serve as "bridges" connecting disparite nodes (or groups of nodes) in a network. More formally, betweenness centrality describes the frequencies of nodes in the shortest paths between indirectly connected nodes (Freeman 1977, 1978). While these measures are distinct, they do correlate pretty highly in our network (r = 0.62), so we should not expect to see major differences between most of the high ranking nodes. Let's take a look at this graph. 

```{r visualizing betweenness centrality}
# creating a color palette for data types 
my_colors <- 'd3.scaleOrdinal() .domain(["1", "2", "3", "4", "5", 
                                         "6", "7", "8", "9", "10",
                                         "11", "12", "13", "14", "15", "16"])
                                .range([ "#005b96", "#851e3e", "#35a79c", "#851e3e", 
                                         "#851e3e", "#851e3e", "#851e3e", "#851e3e", 
                                         "#851e3e", "#851e3e", "#851e3e", "#851e3e", 
                                         "#851e3e", "#851e3e", "#851e3e", "#851e3e"])'
# visualizing the network 
networkD3::forceNetwork(Links = edgeList, Nodes = nodeList, Source = "word1ID", Target = "word2ID", 
                        Value = "weight", NodeID = "nName", Nodesize = "nodeBetweenness",  
                        opacity = 0.9, opacityNoHover = 0.05, height = 500, width = 910,  
                        Group = "cluster", fontSize = 120, fontFamily = "sans-serif",
                        linkDistance = networkD3::JS("function(d) { return 3*d.value; }"),
                        linkWidth = JS("function(d) { return Math.sqrt(d.value / 10); }"), 
                        zoom = TRUE, charge = -20, colourScale = my_colors) 
```

When we look at this graph, we see some similar terms with large node sizes. For example, American and Asian are both still pretty large. Relatively speaking, however, African has diminished in size considerably (was at the center left and now at the right periphery). Some of these changes are easy to catch, but others are not. Thus, it might be wise to analyze the biggest differences in degree centrality to betweenness centrality (in terms of where they rank in our list of terms). Let's take a look at the terms with the biggest differences.

```{r centrality differences, rows.print=50, fig.width=9.5}
nodeList %>% 
  mutate(nodeDegRank = round(rank(-nodeDegree))) %>% 
  mutate(nodeBetwRank = round(rank(-nodeBetweenness))) %>% 
  mutate(centDiff = nodeDegRank - nodeBetwRank) %>% 
  arrange(-centDiff) %>% rename(term = nName) %>% 
  select(term, nodeDegree, nodeDegRank, nodeBetweenness, nodeBetwRank, centDiff)
```

Interesting! This suggests that more specific national level terms have much more betweenness centrality relative to degree centrality while more general racial, geographical and contintental terms are much less important in terms of betweenness centrality. Brandon is not quite sure what this means, as he originally theorized that more general population terms would have *higher* centDiff scores, which would effectively mean they serve as briding concepts between different clusters of terms. [Bail (2016)](https://www.pnas.org/content/pnas/113/42/11823.full.pdf) has shown that this is the case in the context of social advocacy groups and [Seth Long has theorized](https://technaverbascripta.wordpress.com/2015/09/12/some-questions-about-centrality-measurements-in-text-networks/) that betweenness centrality helps tell us more about "slippage" of signification across different contexts.

Yet, this clearly is not the case in the most general sense. Not only do we see *major* drops in the rank of several general terms like north, Africa, African, race and racial, we see major increases in more specific terms like Brazil, Mexican, Australia and Japan. About the only exceptions to these trends are "Asian" and "American", which both show relative increases in betweenness centrality relative to degree centrality, but because they are high ranking nodes in both centrality measures, it is hardly noticeable with this methods. Perhaps, this is method is just inherently flawed or maybe it is better to detect these bridging patterns using bipartite networks in the way that Bail did. Regardless, we have some more thinking to do if we think these analyses actually mean anything or not. 

### References

Hollinger, D. A. (2006). *Postethnic America: Beyond Multiculturalism.* Hachette UK.

Freeman, L. C. (1977). A set of measures of centrality based on betweenness. *Sociometry*, 35-41.

Freeman, L. C. (1978). Centrality in social networks conceptual clarification. *Social Networks*, 1(3), 215-239.

Memisevic, V. (2016). Network visualization – part 6: D3 and R (networkD3). [*R-Bloggers*](https://www.r-bloggers.com/network-visualization-part-6-d3-and-r-networkd3/)

Pons, P., & Latapy, M. (2006). Computing communities in large networks using random walks. *Journal of Graph Algorithms and Applications*, 10(2), 191-218.

Silge, J., & Robinson, D. (2016). tidytext: Text Mining and Analysis Using Tidy Data Principles in R. *Journal of Open Source Software*, 1(3), 37.

### Appendices 

**Appendix 1.** Here is a full list of all the blue nodes from the community detection algorithm.

```{r appendix 1, rows.print=25, fig.width=9.5}
nodeList %>% 
  filter(cluster == "1") %>%
  rename(term = nName) %>% select(-ID)
```

**Appendix 2.** Here is a full list of all the green nodes from the community detection algorithm.

```{r appendix 2, rows.print=25, fig.width=9.5}
nodeList %>% 
  filter(cluster == "3") %>%
  rename(term = nName) %>% select(-ID)
```

**Appendix 3.** Here is a full list of all the deep red nodes from the community detection algorithm.

```{r appendix 3, rows.print=50, fig.width=9.5}
nodeList %>% 
  filter(cluster != "1" & cluster != "3") %>%
  rename(term = nName) %>% select(-ID)
```

**Appendix 4.** Correlation Calculation for Degree and Betweenness Centrality 

```{r appendix 4, fig.width=9.5}
cor(nodeList$nodeDegree, nodeList$nodeBetweenness)
```



























