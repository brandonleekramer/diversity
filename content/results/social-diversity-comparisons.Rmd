---
title: "Social Diversity Comparisons"
description: "Comparisons of classification algorithms differentiating social compared to more general uses of the term diversity"
output: html_document
weight: 5
---

```{css, echo=FALSE}
/* this chunk of code centers all of the headings */
h1, h2, h3, h4, h5 {
  text-align: center;
}
```

```{r, echo=FALSE, warnings=FALSE, messages=FALSE,include=FALSE}
library("tidyverse")
setwd("~/git/diversity/data/sensitivity_checks/")
final_stats_output <- read_csv("socdiv_version_comps_073021.csv")
```

```{r, fig.height=5, fig.align="center", echo=FALSE, warnings=FALSE, messages=FALSE}
final_stats_output %>% 
  mutate(version = str_replace(version, "baseline", "Baseline"),
         version = str_replace(version, "version", "Version"),
         category = str_replace(category, "f1_score", "F1 Score"),
         category = str_replace(category, "accuracy", "Accuracy"),
         category = str_replace(category, "recall", "Recall"),
         category = str_replace(category, "precision", "Precision")
         ) %>% 
  ggplot(aes(x=version, y=measure, group=category)) +
  geom_line(aes(linetype=category))+
  geom_point() +
  theme_minimal() +
  labs(title = "Supplentary Figure 3. Comparison of Social Diversity Classification Algorithms",
       linetype = "Measurements") + 
  theme(legend.position="bottom",
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        plot.title = element_text(hjust = 0.5))
```

<br>

**Automating the Measurement of Social Diversity.** To measure “social diversity” in biomedical abstracts, we developed a series of algorithms to detect the mentions of diversity that reference human forms of diversity rather than general heterogeneity or biodiversity. Our initial specification of this algorithm (the Baseline model) counted any mention of the term diversity that was used in the same abstract as any of the other terms in the other 11 categories. After running these tests, we hand-coded an initial set of 500 abstracts that mention “any diversity” and 500 that mentioned “social diversity” more specifically. This baseline version garnered a very low F1 score - mainly because the precision was only around 46%, meaning that we have an extremely high false-positive rate. To address this limitation, we developed eight different comparison algorithms, making small tweaks in each step, and then having two research assistants hand-code 500 abstracts from each of the “all diversity” and “social diversity” samples. Overall, our inter-rater reliability was ~91% between the two coders with any discrepancies being resolved by a third coder (the second co-author). 

<br>

```{r, warning=FALSE, message=FALSE, echo = FALSE}
setwd("~/git/diversity/data/final_data/")
supp_data <- read_csv("diversity_project - supp_table_3.csv") 
DT::datatable(supp_data, 
          caption = 'Supplementary Table 3: Different Parameters Tested in Social Diversity Classifiers', 
          rownames = FALSE)
```

<br>

The first step we took was to improve our animal exclusion clause. In the Baseline run, we had a fairly small list of 237 animal terms and only removed abstracts if they mentioned animals and did not mention humans. To improve this, we increased the nonhuman list to 1,117 terms. Obviously, this step provides the largest improvement in our testing series (see Baseline to Version 1).Second, we reduced false positives by eradicating common forms of polysemy related to bioecological, cellular, and methodological diversity as well as general forms of heterogeneity. These patterns were extracted using `R`’s `tidytext` package to extract n-grams of “diverse [terms],” “diversity of [terms],” and “[terms] diversity.” Alone, this function makes only a marginal difference, but with the animal exclusion clause do see notable improvements (see Baseline to Version 2). 

<br>

##### Final Confusion Matrix Comparing All Diversity vs. Social Diversity Categories (n=1000)

<center>

|     | Predicted: YES        | Predicted: NO       | 
| :------------: | :------------: | :------------: | 
| <b>Actual: YES</b>   | True Positives = 386        | False Negatives = 24       | 
| <b>Actual: NO</b>   | False Positives = 114         | True Negatives = 476       | 

</center>

<br>

Next, we tried implementing an even more stringent nonhuman exclusion clauses. Rather than removing all abstracts that mention nonhumans and did not mention humans, we kept only abstracts that mention humans and removed all that mention animals in Version 3 while adding the polysemy adjustment function in Version 4. Following a similar logic, we also removed any abstract than mentioned nonhuman terms in Version 5 with a polysemy adjustment in Version 6. Much to our surprise, these versions diminished predictive capacity slightly, but opted to keep this version in our final algorithm to minimize the potential of having false positives in animal studies. 

<br>

<center>

##### Final Performance Metrics for Social Diversity Classifier

| Measure | Outcome |  
| :--------: | :--------: |  
| Precision | 77.2% |
| Recall    | 94.1% |
| Accuracy  | 86.2% |
| F1 Score  | 84.8% |

</center>

<br>

At this point, we recognized that we needed to make a more drastic change in our approach as we were still a bit below the 80% F1 score we had set out to exceed. We decided to “count” only mentions of diversity that co-occur alongside the terms in the other categories in the same sentence. At this point, we also recognized that we needed to add another category that accounts for discussions of “diverse groups” that included terms like groups, cohorts, humans, residents, etc. (i.e. Polysemy Adjustment Version 2). Initially, we included “population(s)” and “subpopulation(s)” into this category, leading to an enormous false positive rate that dropped our accuracy and precision scores back down to near baseline (Version 7). After removing these two “(sub)population(s)” terms from the 12th category, our scores normalized in Version 8 while adding a newer version of the Polysemy Adjustment increased this score even more in Version 9. In the final step, we returned to our most successful version of the Animal Exclusion Clause and ended with an F1 score nearing 85%. Table 2 presents the confusion matrix for this final classifier, showing that our model does tend to inflate false positive cases of social diversity. We advise readers to keep this in mind when interpreting the results of Hypotheses 1-3.











